# Awesome-Segment-Anything-Model

Segment Anything Model Repository: A collection of documents, papers, source code, and talks for Meta AI's Segment Anything Model (SAM) and related studies.

**Keywords:** Segment Anything Model, Segment Anything, SAM, awesome

> **CATALOGUE**
>
>[Origin of the Study](#quick-start) :heartpulse: [Toolbox & Framework](#tool) :heartpulse: [Lecture & Notes](#workshop) :heartpulse: [Papers](#papers-by-categories)

## 1 Origin of the Study <span id='quick-start'></span>

**Fundemental Models**

+ **[SAM]** Segment Anything (2023)[[paper]](https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/10000000_900554171201033_1602411987825904100_n.pdf?_nc_cat=100&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=5R36a2pHTEoAX-N4s7w&_nc_ht=scontent-lax3-2.xx&oh=00_AfAt7-wce1N9DevVZ4-cgxo_lgAI60jIoPc9EDR2P1VORg&oe=643CE9E7) [[Project]](https://github.com/facebookresearch/segment-anything)![GitHub stars](https://img.shields.io/github/stars/facebookresearch/segment-anything.svg?logo=github&label=Stars)

+ **[SEEM]** Segment Everything Everywhere All at Once (2023)[[paper]](https://arxiv.org/pdf/2304.06718.pdf)[[Project]](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)![GitHub stars](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.svg?logo=github&label=Stars)

## 2 Toolbox & Framework<span id='tool'>

+ **[Grounded-Segment-Anything]** Grounded-Segment-Anything[[paper]][[code]](https://github.com/IDEA-Research/Grounded-Segment-Anything)![GitHub stars](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything.svg?logo=github&label=Stars)

+ **[Awesome Segment-Anything Extensions]** Awesome Segment-Anything Extensions[[paper]][[code]](https://github.com/JerryX1110/awesome-segment-anything-extensions)![GitHub stars](https://img.shields.io/github/stars/JerryX1110/awesome-segment-anything-extensions.svg?logo=github&label=Stars)

+ **[SALT]** Segment Anything Labelling Tool[[paper]][[code]](https://github.com/anuragxel/salt)![GitHub stars](https://img.shields.io/github/stars/anuragxel/salt.svg?logo=github&label=Stars)

## 3 Lecture & Notes<span id='workshop'>

**How to | roboflow** how-to-segment-anything-with-sam [[blog]](https://github.com/roboflow/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb)


## 4 Papers <span id='papers-by-categories'></span>
  
### Analysis & Emprical Study

**[Zero-shot Segmentation]** Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging[[paper]](https://arxiv.org/abs/2304.04155)[[code]](https://github.com/BingfengYan/VISAM)![GitHub stars](https://img.shields.io/github/stars/BingfengYan/VISAM.svg?logo=github&label=Stars)

**[Camouflaged Object Segmentation]** SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"[[paper]](https://arxiv.org/abs/2304.06022)[code]

**[Camouflaged Object Detection]** Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection[[paper]](https://arxiv.org/abs/2304.04709)[[code]](https://github.com/luckybird1994/SAMCOD)![GitHub stars](https://img.shields.io/github/stars/luckybird1994/SAMCOD.svg?logo=github&label=Stars)

**[Multi-Object Tracking]** CO-MOT[paper][[code]](https://github.com/BingfengYan/VISAM)![GitHub stars](https://img.shields.io/github/stars/BingfengYan/VISAM.svg?logo=github&label=Stars)

**[Brain Extraction]** Brain Extraction comparing Segment Anything Model (SAM) and FSL Brain Extraction Tool[[paper]](https://arxiv.org/abs/2304.04738)[code]

### Arxiv-daily-update
[Click here](https://github.com/Vision-Intelligence-and-Robots-Group/Awesome-Segment-Anything-Model/blob/main/arxiv-daily-docs/README.md) to check the daily-updated paper list!

