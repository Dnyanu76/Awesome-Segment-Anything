## Updated on 2023.06.25

## SAM

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2023-06-22**|**Ladder Fine-tuning approach for SAM integrating complementary network**|Shurong Chai et.al.|[2306.12737v1](http://arxiv.org/abs/2306.12737v1)|**[link](https://github.com/11yxk/sam-lst)**|
|**2023-06-21**|**Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images**|Mohsen Ahmadi et.al.|[2306.12510v1](http://arxiv.org/abs/2306.12510v1)|null|
|**2023-06-21**|**Fast Segment Anything**|Xu Zhao et.al.|[2306.12156v1](http://arxiv.org/abs/2306.12156v1)|**[link](https://github.com/casia-iva-lab/fastsam)**|
|**2023-06-20**|**Segment Anything Model (SAM) for Radiation Oncology**|Lian Zhang et.al.|[2306.11730v1](http://arxiv.org/abs/2306.11730v1)|null|
|**2023-06-22**|**Enlighten Anything: When Segment Anything Model Meets Low-Light Image Enhancement**|Qihan Zhao et.al.|[2306.10286v3](http://arxiv.org/abs/2306.10286v3)|**[link](https://github.com/QihanZhao/enlighten-anything)**|
|**2023-06-15**|**Winning Solution for the CVPR2023 Visual Anomaly and Novelty Detection Challenge: Multimodal Prompting for Data-centric Anomaly Detection**|Yunkang Cao et.al.|[2306.09067v1](http://arxiv.org/abs/2306.09067v1)|**[link](https://github.com/caoyunkang/segment-any-anomaly)**|
|**2023-06-15**|**Temporally-Extended Prompts Optimization for SAM in Interactive Medical Image Segmentation**|Chuyun Shen et.al.|[2306.08958v1](http://arxiv.org/abs/2306.08958v1)|null|
|**2023-06-14**|**TomoSAM: a 3D Slicer extension using SAM for tomography segmentation**|Federico Semeraro et.al.|[2306.08609v1](http://arxiv.org/abs/2306.08609v1)|**[link](https://github.com/fsemerar/slicertomosam)**|
|**2023-06-13**|**Robustness of SAM: Segment Anything Under Corruptions and Beyond**|Yu Qiao et.al.|[2306.07713v1](http://arxiv.org/abs/2306.07713v1)|null|
|**2023-06-10**|**AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder**|Tal Shaharabany et.al.|[2306.06370v1](http://arxiv.org/abs/2306.06370v1)|null|
|**2023-06-21**|**A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering**|Chaoning Zhang et.al.|[2306.06211v2](http://arxiv.org/abs/2306.06211v2)|null|
|**2023-06-01**|**SAM-helps-Shadow:When Segment Anything Model meet shadow removal**|Xiaofeng Zhang et.al.|[2306.06113v1](http://arxiv.org/abs/2306.06113v1)|**[link](https://github.com/zhangbaijin/sam-helps-shadow)**|
|**2023-06-08**|**Matting Anything**|Jiachen Li et.al.|[2306.05399v1](http://arxiv.org/abs/2306.05399v1)|**[link](https://github.com/shi-labs/matting-anything)**|
|**2023-06-07**|**Matte Anything: Interactive Natural Image Matting with Segment Anything Models**|Jingfeng Yao et.al.|[2306.04121v1](http://arxiv.org/abs/2306.04121v1)|**[link](https://github.com/hustvl/matte-anything)**|
|**2023-06-06**|**SAM3D: Segment Anything in 3D Scenes**|Yunhan Yang et.al.|[2306.03908v1](http://arxiv.org/abs/2306.03908v1)|**[link](https://github.com/pointcept/segmentanything3d)**|
|**2023-06-06**|**Towards Label-free Scene Understanding by Vision Foundation Models**|Runnan Chen et.al.|[2306.03899v1](http://arxiv.org/abs/2306.03899v1)|**[link](https://github.com/runnanchen/label-free-scene-understanding)**|
|**2023-06-05**|**Calib-Anything: Zero-training LiDAR-Camera Extrinsic Calibration Method Using Segment Anything**|Zhaotong Luo et.al.|[2306.02656v1](http://arxiv.org/abs/2306.02656v1)|**[link](https://github.com/opencalib/calibanything)**|
|**2023-06-06**|**3rd Place Solution for PVUW2023 VSS Track: A Large Model for Semantic Segmentation on VSPW**|Shijie Chang et.al.|[2306.02291v2](http://arxiv.org/abs/2306.02291v2)|**[link](https://github.com/dut-csj/pvuw2023-vss-3rd)**|
|**2023-06-04**|**USD: Unknown Sensitive Detector Empowered by Decoupled Objectness and Segment Anything Model**|Yulin He et.al.|[2306.02275v1](http://arxiv.org/abs/2306.02275v1)|null|
|**2023-06-04**|**SAM3D: Zero-Shot 3D Object Detection via Segment Anything Model**|Dingyuan Zhang et.al.|[2306.02245v1](http://arxiv.org/abs/2306.02245v1)|**[link](https://github.com/dyzhang09/sam3d)**|
|**2023-06-03**|**Segment Anything Meets Semantic Communication**|Shehbaz Tariq et.al.|[2306.02094v1](http://arxiv.org/abs/2306.02094v1)|null|
|**2023-06-02**|**Segment Anything in High Quality**|Lei Ke et.al.|[2306.01567v1](http://arxiv.org/abs/2306.01567v1)|**[link](https://github.com/syscv/sam-hq)**|
|**2023-06-01**|**DeSAM: Decoupling Segment Anything Model for Generalizable Medical Image Segmentation**|Yifan Gao et.al.|[2306.00499v1](http://arxiv.org/abs/2306.00499v1)|**[link](https://github.com/yifangao112/desam)**|
|**2023-06-01**|**Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards**|Guian Fang et.al.|[2305.19599v2](http://arxiv.org/abs/2305.19599v2)|**[link](https://github.com/enderfga/finerewards)**|
|**2023-05-29**|**InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions**|Qian Wang et.al.|[2305.18047v1](http://arxiv.org/abs/2305.18047v1)|**[link](https://github.com/qianwangx/instructedit)**|
|**2023-05-28**|**AIMS: All-Inclusive Multi-Level Segmentation**|Lu Qi et.al.|[2305.17768v1](http://arxiv.org/abs/2305.17768v1)|**[link](https://github.com/dvlab-research/Entity)**|
|**2023-05-26**|**Detect Any Shadow: Segment Anything for Video Shadow Detection**|Yonghui Wang et.al.|[2305.16698v1](http://arxiv.org/abs/2305.16698v1)|**[link](https://github.com/harrytea/detect-anyshadow)**|
|**2023-05-25**|**Interactive Segment Anything NeRF with Feature Imitation**|Xiaokang Chen et.al.|[2305.16233v1](http://arxiv.org/abs/2305.16233v1)|null|
|**2023-05-25**|**On the Robustness of Segment Anything**|Yihao Huang et.al.|[2305.16220v1](http://arxiv.org/abs/2305.16220v1)|null|
|**2023-05-24**|**SAMScore: A Semantic Structural Similarity Metric for Image Translation Evaluation**|Yunxiang Li et.al.|[2305.15367v1](http://arxiv.org/abs/2305.15367v1)|**[link](https://github.com/kent0n-li/samscore)**|
|**2023-05-23**|**SAD: Segment Any RGBD**|Jun Cen et.al.|[2305.14207v1](http://arxiv.org/abs/2305.14207v1)|**[link](https://github.com/jun-cen/segmentanyrgbd)**|
|**2023-05-23**|**A Dive into SAM Prior in Image Restoration**|Zeyu Xiao et.al.|[2305.13620v1](http://arxiv.org/abs/2305.13620v1)|null|
|**2023-05-22**|**Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching**|Yang Liu et.al.|[2305.13310v1](http://arxiv.org/abs/2305.13310v1)|**[link](https://github.com/aim-uofa/matcher)**|
|**2023-05-22**|**Restore Anything Pipeline: Segment Anything Meets Image Restoration**|Jiaxi Jiang et.al.|[2305.13093v1](http://arxiv.org/abs/2305.13093v1)|**[link](https://github.com/eth-siplab/rap)**|
|**2023-05-22**|**UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model**|Zhenghao Zhang et.al.|[2305.12659v1](http://arxiv.org/abs/2305.12659v1)|null|
|**2023-05-21**|**BreastSAM: A Study of Segment Anything Model for Breast Tumor Detection in Ultrasound Images**|Mingzhe Hu et.al.|[2305.12447v1](http://arxiv.org/abs/2305.12447v1)|null|
|**2023-05-19**|**When SAM Meets Shadow Detection**|Leiping Jie et.al.|[2305.11513v1](http://arxiv.org/abs/2305.11513v1)|**[link](https://github.com/leipingjie/samshadow)**|
|**2023-05-24**|**Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model**|Siyuan Huang et.al.|[2305.11176v3](http://arxiv.org/abs/2305.11176v3)|**[link](https://github.com/opengvlab/instruct2act)**|
|**2023-05-18**|**Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping**|Chunming He et.al.|[2305.11003v1](http://arxiv.org/abs/2305.11003v1)|null|
|**2023-05-18**|**Segment Any Anomaly without Training via Hybrid Prompt Regularization**|Yunkang Cao et.al.|[2305.10724v1](http://arxiv.org/abs/2305.10724v1)|**[link](https://github.com/caoyunkang/segment-any-anomaly)**|
|**2023-05-24**|**OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields**|Youtan Yin et.al.|[2305.10503v2](http://arxiv.org/abs/2305.10503v2)|**[link](https://github.com/cuteyyt/or-nerf)**|
|**2023-05-17**|**PromptUNet: Toward Interactive Medical Image Segmentation**|Junde Wu et.al.|[2305.10300v1](http://arxiv.org/abs/2305.10300v1)|**[link](https://github.com/wujunde/promptunet)**|
|**2023-05-17**|**Explain Any Concept: Segment Anything Meets Concept-Based Explanation**|Ao Sun et.al.|[2305.10289v1](http://arxiv.org/abs/2305.10289v1)|null|
|**2023-05-17**|**SAM for Poultry Science**|Xiao Yang et.al.|[2305.10254v1](http://arxiv.org/abs/2305.10254v1)|null|
|**2023-05-22**|**Leaf Only SAM: A Segment Anything Pipeline for Zero-Shot Automated Leaf Segmentation**|Dominic Williams et.al.|[2305.09418v2](http://arxiv.org/abs/2305.09418v2)|**[link](https://github.com/dom3442/leafonlysam)**|
|**2023-05-19**|**A Comprehensive Survey on Segment Anything Model for Vision and Beyond**|Chunhui Zhang et.al.|[2305.08196v2](http://arxiv.org/abs/2305.08196v2)|**[link](https://github.com/liliu-avril/Awesome-Segment-Anything)**|
|**2023-05-15**|**Knowledge distillation with Segment Anything (SAM) model for Planetary Geological Mapping**|Sahib Julka et.al.|[2305.07586v2](http://arxiv.org/abs/2305.07586v2)|null|
|**2023-05-11**|**Segment and Track Anything**|Yangming Cheng et.al.|[2305.06558v1](http://arxiv.org/abs/2305.06558v1)|**[link](https://github.com/z-x-yang/segment-and-track-anything)**|
|**2023-05-12**|**Can SAM Boost Video Super-Resolution?**|Zhihe Lu et.al.|[2305.06524v2](http://arxiv.org/abs/2305.06524v2)|null|
|**2023-05-23**|**An Empirical Study on the Robustness of the Segment Anything Model (SAM)**|Yuqing Wang et.al.|[2305.06422v2](http://arxiv.org/abs/2305.06422v2)|null|
|**2023-05-09**|**Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation**|Tianle Chen et.al.|[2305.05803v1](http://arxiv.org/abs/2305.05803v1)|**[link](https://github.com/cskyl/sam_wsss)**|
|**2023-06-21**|**How Segment Anything Model (SAM) Boost Medical Image Segmentation: A Survey**|Yichi Zhang et.al.|[2305.03678v2](http://arxiv.org/abs/2305.03678v2)|**[link](https://github.com/yichizhang98/sam4mis)**|
|**2023-05-05**|**BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks**|Zihan Guan et.al.|[2305.03289v1](http://arxiv.org/abs/2305.03289v1)|null|
|**2023-05-04**|**Personalize Segment Anything Model with One Shot**|Renrui Zhang et.al.|[2305.03048v1](http://arxiv.org/abs/2305.03048v1)|**[link](https://github.com/zrrskywalker/personalize-sam)**|
|**2023-05-08**|**Caption Anything: Interactive Image Description with Diverse Multimodal Controls**|Teng Wang et.al.|[2305.02677v2](http://arxiv.org/abs/2305.02677v2)|**[link](https://github.com/ttengwang/caption-anything)**|
|**2023-05-03**|**Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model**|Di Wang et.al.|[2305.02034v1](http://arxiv.org/abs/2305.02034v1)|**[link](https://github.com/vitae-transformer/samrs)**|
|**2023-05-03**|**AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation**|Shentong Mo et.al.|[2305.01836v1](http://arxiv.org/abs/2305.01836v1)|null|
|**2023-05-02**|**An Alternative to WSSS? An Empirical Study of the Segment Anything Model (SAM) on Weakly-Supervised Semantic Segmentation Problems**|Weixuan Sun et.al.|[2305.01586v1](http://arxiv.org/abs/2305.01586v1)|null|
|**2023-05-02**|**Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation**|Peng-Tao Jiang et.al.|[2305.01275v1](http://arxiv.org/abs/2305.01275v1)|null|
|**2023-05-08**|**Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples**|Chenshuang Zhang et.al.|[2305.00866v2](http://arxiv.org/abs/2305.00866v2)|null|
|**2023-04-29**|**Polyp-SAM: Transfer SAM for Polyp Segmentation**|Yuheng Li et.al.|[2305.00293v1](http://arxiv.org/abs/2305.00293v1)|null|
|**2023-04-29**|**Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected**|Dongsheng Han et.al.|[2305.00278v1](http://arxiv.org/abs/2305.00278v1)|null|
|**2023-04-28**|**DSEC-MOS: Segment Any Moving Object with Moving Ego Vehicle**|Zhuyun Zhou et.al.|[2305.00126v1](http://arxiv.org/abs/2305.00126v1)|**[link](https://github.com/zzy-zhou/dsec-mos)**|
|**2023-05-05**|**Zero-shot performance of the Segment Anything Model (SAM) in 2D medical imaging: A comprehensive evaluation and practical guidelines**|Christian Mattjie et.al.|[2305.00109v2](http://arxiv.org/abs/2305.00109v2)|null|
|**2023-04-28**|**SAM on Medical Images: A Comprehensive Study on Three Prompt Modes**|Dongjie Cheng et.al.|[2305.00035v1](http://arxiv.org/abs/2305.00035v1)|null|
|**2023-04-28**|**SAM Meets Robotic Surgery: An Empirical Study in Robustness Perspective**|An Wang et.al.|[2304.14674v1](http://arxiv.org/abs/2304.14674v1)|null|
|**2023-05-19**|**Segment Anything Model for Medical Images?**|Yuhao Huang et.al.|[2304.14660v4](http://arxiv.org/abs/2304.14660v4)|null|
|**2023-04-27**|**Edit Everything: A Text-Guided Generative System for Images Editing**|Defeng Xie et.al.|[2304.14006v1](http://arxiv.org/abs/2304.14006v1)|**[link](https://github.com/defengxie/edit_everything)**|
|**2023-04-27**|**SkinSAM: Empowering Skin Cancer Segmentation with Segment Anything Model**|Mingzhe Hu et.al.|[2304.13973v1](http://arxiv.org/abs/2304.13973v1)|null|
|**2023-04-26**|**GazeSAM: What You See is What You Segment**|Bin Wang et.al.|[2304.13844v1](http://arxiv.org/abs/2304.13844v1)|**[link](https://github.com/ukaukaaaa/gazesam)**|
|**2023-04-26**|**Customized Segment Anything Model for Medical Image Segmentation**|Kaidong Zhang et.al.|[2304.13785v1](http://arxiv.org/abs/2304.13785v1)|**[link](https://github.com/hitachinsk/samed)**|
|**2023-04-26**|**Learnable Ophthalmology SAM**|Zhongxi Qiu et.al.|[2304.13425v1](http://arxiv.org/abs/2304.13425v1)|**[link](https://github.com/qsingle/learnablepromptsam)**|
|**2023-05-15**|**Segment anything, from space?**|Simiao Ren et.al.|[2304.13000v2](http://arxiv.org/abs/2304.13000v2)|null|
|**2023-04-25**|**Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation**|Peilun Shi et.al.|[2304.12637v1](http://arxiv.org/abs/2304.12637v1)|null|
|**2023-05-13**|**Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation**|Junde Wu et.al.|[2304.12620v6](http://arxiv.org/abs/2304.12620v6)|**[link](https://github.com/wujunde/medical-sam-adapter)**|
|**2023-04-25**|**Application of Segment Anything Model for Civil Infrastructure Defect Assessment**|Mohsen Ahmadi et.al.|[2304.12600v1](http://arxiv.org/abs/2304.12600v1)|**[link](https://github.com/Gunorta/SegmentAnything_CrackDetection)**|
|**2023-04-26**|**Segment Anything in 3D with NeRFs**|Jiazhong Cen et.al.|[2304.12308v2](http://arxiv.org/abs/2304.12308v2)|null|
|**2023-04-24**|**Segment Anything in Medical Images**|Jun Ma et.al.|[2304.12306v1](http://arxiv.org/abs/2304.12306v1)|**[link](https://github.com/bowang-lab/medsam)**|
|**2023-04-28**|**Track Anything: Segment Anything Meets Videos**|Jinyu Yang et.al.|[2304.11968v2](http://arxiv.org/abs/2304.11968v2)|**[link](https://github.com/gaomingqi/track-anything)**|
|**2023-04-23**|**Segment Anything in Non-Euclidean Domains: Challenges and Opportunities**|Yongcheng Jing et.al.|[2304.11595v1](http://arxiv.org/abs/2304.11595v1)|null|
|**2023-04-22**|**Input Augmentation with SAM: Boosting Medical Image Segmentation with Segmentation Foundation Model**|Yizhe Zhang et.al.|[2304.11332v1](http://arxiv.org/abs/2304.11332v1)|null|
|**2023-04-21**|**Can SAM Count Anything? An Empirical Study on SAM Counting**|Zhiheng Ma et.al.|[2304.10817v1](http://arxiv.org/abs/2304.10817v1)|**[link](https://github.com/vision-intelligence-and-robots-group/count-anything)**|
|**2023-04-20**|**Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models**|Jielu Zhang et.al.|[2304.10597v1](http://arxiv.org/abs/2304.10597v1)|**[link](https://github.com/douglas2code/text2seg)**|
|**2023-05-17**|**Segment Anything Model for Medical Image Analysis: an Experimental Study**|Maciej A. Mazurowski et.al.|[2304.10517v3](http://arxiv.org/abs/2304.10517v3)|**[link](https://github.com/mazurowski-lab/segment-anything-medical-evaluation)**|
|**2023-04-19**|**Anything-3D: Towards Single-view Anything Reconstruction in the Wild**|Qiuhong Shen et.al.|[2304.10261v1](http://arxiv.org/abs/2304.10261v1)|**[link](https://github.com/anything-of-anything/anything-3d)**|
|**2023-04-20**|**Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate**|Songhua Liu et.al.|[2304.09728v2](http://arxiv.org/abs/2304.09728v2)|**[link](https://github.com/huage001/transfer-any-style)**|
|**2023-05-05**|**Computer-Vision Benchmark Segment-Anything Model (SAM) in Medical Images: Accuracy in 12 Datasets**|Sheng He et.al.|[2304.09324v3](http://arxiv.org/abs/2304.09324v3)|null|
|**2023-05-02**|**SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More**|Tianrun Chen et.al.|[2304.09148v3](http://arxiv.org/abs/2304.09148v3)|null|
|**2023-05-09**|**When SAM Meets Medical Images: An Investigation of Segment Anything Model (SAM) on Multi-phase Liver Tumor Segmentation**|Chuanfei Hu et.al.|[2304.08506v4](http://arxiv.org/abs/2304.08506v4)|null|
|**2023-04-17**|**Learning to "Segment Anything" in Thermal Infrared Images through Knowledge Distillation with a Large Scale Dataset SATIR**|Junzhang Chen et.al.|[2304.07969v1](http://arxiv.org/abs/2304.07969v1)|**[link](https://github.com/chenjzbuaa/satir)**|
|**2023-04-16**|**The Segment Anything foundation model achieves favorable brain tumor autosegmentation accuracy on MRI to support radiotherapy treatment planning**|Florian Putz et.al.|[2304.07875v1](http://arxiv.org/abs/2304.07875v1)|null|
|**2023-04-16**|**Deep learning universal crater detection using Segment Anything Model (SAM)**|Iraklis Giannakis et.al.|[2304.07764v1](http://arxiv.org/abs/2304.07764v1)|null|
|**2023-04-15**|**Can SAM Segment Polyps?**|Tao Zhou et.al.|[2304.07583v1](http://arxiv.org/abs/2304.07583v1)|null|
|**2023-04-13**|**Inpaint Anything: Segment Anything Meets Image Inpainting**|Tao Yu et.al.|[2304.06790v1](http://arxiv.org/abs/2304.06790v1)|**[link](https://github.com/geekyutao/inpaint-anything)**|
|**2023-04-27**|**SAM Struggles in Concealed Scenes -- Empirical Study on "Segment Anything"**|Ge-Peng Ji et.al.|[2304.06022v3](http://arxiv.org/abs/2304.06022v3)|null|
|**2023-04-13**|**Segment Anything Is Not Always Perfect: An Investigation of SAM on Different Real-world Applications**|Wei Ji et.al.|[2304.05750v2](http://arxiv.org/abs/2304.05750v2)|null|
|**2023-04-12**|**CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks**|Yi Li et.al.|[2304.05653v1](http://arxiv.org/abs/2304.05653v1)|**[link](https://github.com/xmed-lab/clip_surgery)**|
|**2023-04-12**|**SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM**|Yihao Liu et.al.|[2304.05622v1](http://arxiv.org/abs/2304.05622v1)|**[link](https://github.com/bingogome/samm)**|
|**2023-04-10**|**SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model**|Saikat Roy et.al.|[2304.05396v1](http://arxiv.org/abs/2304.05396v1)|null|
|**2023-04-19**|**SAM vs BET: A Comparative Study for Brain Extraction and Segmentation of Magnetic Resonance Images using Deep Learning**|Sovesh Mohapatra et.al.|[2304.04738v3](http://arxiv.org/abs/2304.04738v3)|null|
|**2023-04-11**|**Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection**|Lv Tang et.al.|[2304.04709v2](http://arxiv.org/abs/2304.04709v2)|**[link](https://github.com/luckybird1994/samcod)**|
|**2023-04-09**|**Segment Anything Model (SAM) for Digital Pathology: Assess Zero-shot Segmentation on Whole Slide Imaging**|Ruining Deng et.al.|[2304.04155v1](http://arxiv.org/abs/2304.04155v1)|null|
|**2023-04-05**|**Segment Anything**|Alexander Kirillov et.al.|[2304.02643v1](http://arxiv.org/abs/2304.02643v1)|**[link](https://github.com/facebookresearch/segment-anything)**|
|**2020-04-01**|**Towards Segmenting Anything That Moves**|Achal Dave et.al.|[1902.03715v4](http://arxiv.org/abs/1902.03715v4)|null|

